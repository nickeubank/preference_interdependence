{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating Preference Independence using Regression Trees\n",
    "\n",
    "From prior analysis, we have reason to believe that the causal impact of different conditions in a conjoint experiment on support for business development investment may not be independent. In particular, there is evidence that when one varies the potential economic returns to the investment, one sees very different patterns in the causal effects of different information treatments related to the environmental impact of these projects. \n",
    "\n",
    "To illustrate, consider the following two plots. In the first, we show how the causal effect of different job-creation treatments does not vary with environmental certification treatments:\n",
    "\n",
    "![no_dependency](40_docs/images/no_dependency.png)![legend](40_docs/images/legend.png)\n",
    "\n",
    "As the figure shows, regardess of whether the investment is said to be pursuing or is said to have received environmental certification (i.e. across the three certification treatments), the marginal impact of moving from 100 jobs to 1000 jobs is constant, and the marginal impact of moving from 1,000 to 100,000 jobs is constant. \n",
    "\n",
    "My contrast, if we look at the marginal effect of different waste treatment conditions, the marginal effects vary radically. \n",
    "\n",
    "![dependent_prefs](40_docs/images/dependent_prefs.png)![legend](40_docs/images/legend.png)\n",
    "\n",
    "If waste is low (100HH), then moving from 100 jobs to 1,000 jobs has about the same marginal effect as moving from 1,000 to 100,000. If, by contrast, there is waste 300HH of waste, then the marginal effect of moving from 100 to 1,000 jobs is large, but there's no effect of moving from 1,000 to 100,000. And if there is 1,000 HH of waste, then moving from 100 to 1,000 jobs has no effect, but moving from 1,000 to 100,000 jobs has a large effect. \n",
    "\n",
    "This is evidence of interdependence of effects. In addition (though this is not in the paper), this interdependence appears to be asymmetric: while the waste impact of a project impacts the effect of different job sizes, it is not clear the reverse is true (**right? Can't quite remember what slide you showed me!**)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and Clean Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import pandas as pd\n",
    "os.chdir('/users/nick/github/preference_interdependence')\n",
    "df = pd.read_stata('00_source_data/Fish_VN2018_rep.dta')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Just lookin' around...\n",
    "df.sample(5).transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subset to core treatments\n",
    "df = df[df.columns[3:12].to_list() + ['choice']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert from a heterogeneous table to a homogenous array \n",
    "# (scikit-learn hates heterogeneous tables)\n",
    "\n",
    "# Build formula for converting categoricals \n",
    "# to one-hot vectors\n",
    "import patsy\n",
    "formula = ''\n",
    "for i in df.columns[:-1]:\n",
    "    formula = formula + f'C({i}) + '\n",
    "formula[:-2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Move categoricals to one-hot encodings (i.e. indicator variables)\n",
    "df_y , df_X = patsy.dmatrices('choice ~ ' + formula[:-2], df, return_type='dataframe')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data for training and testing\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(df_X.values, df_y.values, \n",
    "                                                    test_size=0.5, \n",
    "                                                    train_size=0.5,\n",
    "                                                    random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import tree\n",
    "\n",
    "my_model = tree.DecisionTreeClassifier(max_depth=3)\n",
    "my_model.fit(X_train, y_train)\n",
    "#tree.plot_tree(my_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = tree.export_text(my_model, feature_names=df_X.columns.tolist())\n",
    "print(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import graphviz\n",
    "dot_data = tree.export_graphviz(my_model, out_file=None, \n",
    "                                feature_names=df_X.columns.tolist())\n",
    "graph = graphviz.Source(dot_data)  \n",
    "graph "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
